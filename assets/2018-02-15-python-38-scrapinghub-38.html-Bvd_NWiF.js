import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as n,o as e}from"./app-uJE4P3e0.js";const p={};function l(t,s){return e(),a("div",null,s[0]||(s[0]=[n(`<h1 id="scrapinghub" tabindex="-1"><a class="header-anchor" href="#scrapinghub"><span>scrapinghub</span></a></h1><p><a href="https://scrapinghub.com/" target="_blank" rel="noopener noreferrer">scrapinghub</a> 是一款爬虫托管平台。</p><h2 id="数据的价值" tabindex="-1"><a class="header-anchor" href="#数据的价值"><span>数据的价值</span></a></h2><p>更好的数据可带来更好的决策。</p><p>关注行业趋势，深入了解客户（和竞争对手），保护您的业务 - 可能性无穷无尽。</p><h2 id="按需数据" tabindex="-1"><a class="header-anchor" href="#按需数据"><span>按需数据</span></a></h2><p>如果您没有时间或专业知识来抓取网站，我们的网络抓取专家可以提供帮助。</p><p>你将掌握得很好。 我们是Scrapy的创建者和主要维护者，Scrapy是用Python编写的最流行的Web抓取框架。</p><p>有些网站非常受欢迎或难以抓取，我们会自行收集数据，因此您无需这样做。</p><p>如果您正在考虑抓取此类网站，请与我们联系。</p><p>我们很有机会参与其中。 您可以即时访问所需的数据 - 无需麻烦。</p><p>其他网站很复杂。 在收集所需数据时，Bot对策，草率代码，A / B测试和其他挑战可能会妨碍您。</p><p>我们的专家知道如何解决这些问题。</p><p>通过让我们为您处理那些复杂的爬行来节省时间和金钱</p><h2 id="大规模抓取网页" tabindex="-1"><a class="header-anchor" href="#大规模抓取网页"><span>大规模抓取网页</span></a></h2><p>Scrapy Cloud是我们基于云的Web爬网平台，允许您轻松部署爬网程序并按需扩展它们 - 无需担心服务器，监视，备份或cron作业。</p><p>它可以帮助像您这样的开发人员每月将20亿个网页转换为有价值的数据。</p><p>我们平台的众多附加组件可让您通过点击扩展蜘蛛。</p><p>其中，我们的智能代理旋转器（Crawlera）可帮助您绕过机器人对策，以便您可以更快地抓取大型站点。</p><p>您的数据安全地存储在高可用性数据库中。</p><p>您可以在仪表板中浏览并与团队共享，也可以使用我们的API在您的应用中使用您的数据。</p><h1 id="创建一个-scrapy-项目" tabindex="-1"><a class="header-anchor" href="#创建一个-scrapy-项目"><span>创建一个 scrapy 项目</span></a></h1><h2 id="创建运行项目" tabindex="-1"><a class="header-anchor" href="#创建运行项目"><span>创建运行项目</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>scrapy startproject tutorial</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>日志如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>New Scrapy project &#39;tutorial&#39;, using template directory &#39;c:\\users\\binbin.hou\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scrapy\\templates\\project&#39;, created in:</span></span>
<span class="line"><span>    D:\\python\\tutorial</span></span>
<span class="line"><span></span></span>
<span class="line"><span>You can start your first spider with:</span></span>
<span class="line"><span>    cd tutorial</span></span>
<span class="line"><span>    scrapy genspider example example.com</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="查看目录结构" tabindex="-1"><a class="header-anchor" href="#查看目录结构"><span>查看目录结构</span></a></h3><p>打开 tutorial 文件夹</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>tutorial/</span></span>
<span class="line"><span>    scrapy.cfg            # deploy configuration file</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    tutorial/             # project&#39;s Python module, you&#39;ll import your code from here</span></span>
<span class="line"><span>        __init__.py</span></span>
<span class="line"><span></span></span>
<span class="line"><span>        items.py          # project items definition file</span></span>
<span class="line"><span></span></span>
<span class="line"><span>        middlewares.py    # project middlewares file</span></span>
<span class="line"><span></span></span>
<span class="line"><span>        pipelines.py      # project pipelines file</span></span>
<span class="line"><span></span></span>
<span class="line"><span>        settings.py       # project settings file</span></span>
<span class="line"><span></span></span>
<span class="line"><span>        spiders/          # a directory where you&#39;ll later put your spiders</span></span>
<span class="line"><span>            __init__.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="第一个爬虫" tabindex="-1"><a class="header-anchor" href="#第一个爬虫"><span>第一个爬虫</span></a></h2><p>新建一个文件 <code>blog_spider.py</code> 放在文件夹 <code>tutorial/spiders</code>下。</p><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> scrapy</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">class</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> BlogSpider</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">scrapy</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">Spider</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">)</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;blogspider&#39;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    start_urls </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;https://blog.csdn.net/ryo1060732496&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    next_pages </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> []</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> num </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> range</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">22</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        next_pages.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">append</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;https://blog.csdn.net/ryo1060732496/article/list/&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(num))</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> parse</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> response</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> href </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> response.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">css</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;.article-list&gt;.article-item-box&gt;h4&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">            yield</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;href&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: href.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">css</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;a&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).attrib[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;href&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> next_page </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;"> self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.next_pages:</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">            yield</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> response.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">follow</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(next_page, </span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.parse)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="运行脚本" tabindex="-1"><a class="header-anchor" href="#运行脚本"><span>运行脚本</span></a></h2><p>在项目的顶层执行。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>$   pwd</span></span>
<span class="line"><span>D:\\python\\tutorial</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>执行</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>scrapy crawl blogspider</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h2 id="保存抓取的数据" tabindex="-1"><a class="header-anchor" href="#保存抓取的数据"><span>保存抓取的数据</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>scrapy crawl blogspider -o blog.json</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h1 id="部署到-hub-快速开始" tabindex="-1"><a class="header-anchor" href="#部署到-hub-快速开始"><span>部署到 hub 快速开始</span></a></h1><h2 id="注册登录" tabindex="-1"><a class="header-anchor" href="#注册登录"><span>注册登录</span></a></h2><p>此处我直接使用 github 进行登录。</p><h2 id="scrapy-cloud-projects" tabindex="-1"><a class="header-anchor" href="#scrapy-cloud-projects"><span>Scrapy Cloud Projects</span></a></h2><p>点击新建，创建一个新的爬虫项目。</p><h2 id="code" tabindex="-1"><a class="header-anchor" href="#code"><span>Code</span></a></h2><p>在 hub 上，可以查看对应的项目信息</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>$ pip install shub</span></span>
<span class="line"><span>$ shub login</span></span>
<span class="line"><span>API key: XXXX</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后一直处于登录状态</p><h2 id="deploys" tabindex="-1"><a class="header-anchor" href="#deploys"><span>Deploys</span></a></h2><p>选择在对应的文件目录下，执行部署命令。</p><h3 id="路径确认" tabindex="-1"><a class="header-anchor" href="#路径确认"><span>路径确认</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>&gt; pwd</span></span>
<span class="line"><span>D:\\python\\tutorial</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&gt; ls</span></span>
<span class="line"><span>d-----        2019/4/13     19:07                tutorial</span></span>
<span class="line"><span>-a----        2019/4/13     19:07            259 scrapy.cfg</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="执行部署" tabindex="-1"><a class="header-anchor" href="#执行部署"><span>执行部署</span></a></h3><p>运行部署命令：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>shub deploy 385369</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>部署日志</li></ul><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>Messagepack is not available, please ensure that msgpack-python library is properly installed.</span></span>
<span class="line"><span>Saving project 385369 as default target. You can deploy to it via &#39;shub deploy&#39; from now on</span></span>
<span class="line"><span>Saved to D:\\python\\tutorial\\scrapinghub.yml.</span></span>
<span class="line"><span>Packing version 1555154269</span></span>
<span class="line"><span>Created setup.py at D:\\python\\tutorial</span></span>
<span class="line"><span>Deploying to Scrapy Cloud project &quot;385369&quot;</span></span>
<span class="line"><span>Run your spiders at: https://app.scrapinghub.com/p/385369/</span></span>
<span class="line"><span>{&quot;status&quot;: &quot;ok&quot;, &quot;project&quot;: 385369, &quot;version&quot;: &quot;1555154269&quot;, &quot;spiders&quot;: 1}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="hub-页面查看" tabindex="-1"><a class="header-anchor" href="#hub-页面查看"><span>Hub 页面查看</span></a></h2><p>直接打开连接 <a href="https://app.scrapinghub.com/p/385369/1" target="_blank" rel="noopener noreferrer">https://app.scrapinghub.com/p/385369/1</a> 可以看到刚才部署的爬虫。</p><p>直接点击右上角的【run】</p><p>日志如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span></span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="爬虫项目运行" tabindex="-1"><a class="header-anchor" href="#爬虫项目运行"><span>爬虫项目运行</span></a></h3><p>看的出来，可以可视化的指定优先级，可以执行很多爬虫。</p><h2 id="定时执行" tabindex="-1"><a class="header-anchor" href="#定时执行"><span>定时执行</span></a></h2><p>scrapinghub还有一个强大的功能就是定时执行爬虫任务，一般我们的需求就是每天定时爬取某个站点来获取更新的数据，刚好定时任务就派上用场了。</p><p>在scrapinghub中创建定时任务也非常的简单。</p><p>在菜单栏左侧点击 Periodic Jobs，就进入到定时任务面板了。</p><h2 id="数据" tabindex="-1"><a class="header-anchor" href="#数据"><span>数据</span></a></h2><p>可以把收集的数据进行导出。</p><h1 id="个人收获" tabindex="-1"><a class="header-anchor" href="#个人收获"><span>个人收获</span></a></h1><p>这种类似的平台其实非常棒</p><ol><li><p>但是前提是要有钱。</p></li><li><p>比如 github 在初期肯定需要大量的资金，后期如果做得比较好，就可以利用大量的用户来换取巨额投资。</p></li></ol><h2 id="数据-1" tabindex="-1"><a class="header-anchor" href="#数据-1"><span>数据</span></a></h2><p>常言道数据无价，谨慎操作。</p><p>这个 hub 可以获取到程序员的大量爬虫结果。</p><h2 id="速度" tabindex="-1"><a class="header-anchor" href="#速度"><span>速度</span></a></h2><p>不知道是不是网速的原因，感觉脚本执行的特别慢。</p><p>本地几秒钟的执行脚本，执行了几分钟还没有结束。</p><h2 id="技术的本质" tabindex="-1"><a class="header-anchor" href="#技术的本质"><span>技术的本质</span></a></h2><p>技术的本质还是一样的。</p><p>定时 job 任务的执行。</p><h1 id="拓展阅读" tabindex="-1"><a class="header-anchor" href="#拓展阅读"><span>拓展阅读</span></a></h1><p><a href="">多线程</a></p><p><a href="">yeild 关键字</a></p><p><a href="">python 生成器</a></p><h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h1><p><a href="https://www.jianshu.com/p/27e5897bc95b" target="_blank" rel="noopener noreferrer">数据采集练习之部署爬虫到Scrapy Cloud</a></p><ul><li>scrapy</li></ul><p><a href="http://doc.scrapy.org/en/latest/intro/tutorial.html" target="_blank" rel="noopener noreferrer">http://doc.scrapy.org/en/latest/intro/tutorial.html</a></p>`,90)]))}const d=i(p,[["render",l]]),c=JSON.parse('{"path":"/posts/Lang/2018-02-15-python-38-scrapinghub-38.html","title":"Python-38-scrapinghub 爬虫平台","lang":"zh-CN","frontmatter":{"title":"Python-38-scrapinghub 爬虫平台","date":"2018-02-14T00:00:00.000Z","categories":["Lang"],"tags":["python","python3","splider","lang","sh"],"published":true,"description":"scrapinghub scrapinghub 是一款爬虫托管平台。 数据的价值 更好的数据可带来更好的决策。 关注行业趋势，深入了解客户（和竞争对手），保护您的业务 - 可能性无穷无尽。 按需数据 如果您没有时间或专业知识来抓取网站，我们的网络抓取专家可以提供帮助。 你将掌握得很好。 我们是Scrapy的创建者和主要维护者，Scrapy是用Pytho...","head":[["meta",{"property":"og:url","content":"https://houbb.github.io/blog-backend/posts/Lang/2018-02-15-python-38-scrapinghub-38.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"Python-38-scrapinghub 爬虫平台"}],["meta",{"property":"og:description","content":"scrapinghub scrapinghub 是一款爬虫托管平台。 数据的价值 更好的数据可带来更好的决策。 关注行业趋势，深入了解客户（和竞争对手），保护您的业务 - 可能性无穷无尽。 按需数据 如果您没有时间或专业知识来抓取网站，我们的网络抓取专家可以提供帮助。 你将掌握得很好。 我们是Scrapy的创建者和主要维护者，Scrapy是用Pytho..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-08-22T03:45:14.000Z"}],["meta",{"property":"article:tag","content":"python"}],["meta",{"property":"article:tag","content":"python3"}],["meta",{"property":"article:tag","content":"splider"}],["meta",{"property":"article:tag","content":"lang"}],["meta",{"property":"article:tag","content":"sh"}],["meta",{"property":"article:published_time","content":"2018-02-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-22T03:45:14.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Python-38-scrapinghub 爬虫平台\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2018-02-14T00:00:00.000Z\\",\\"dateModified\\":\\"2025-08-22T03:45:14.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"]]},"git":{"createdTime":1755834314000,"updatedTime":1755834314000,"contributors":[{"name":"binbin.hou","username":"","email":"binbin.hou@huifu.com","commits":1}]},"readingTime":{"minutes":4.36,"words":1309},"filePathRelative":"posts/Lang/2018-02-15-python-38-scrapinghub-38.md","localizedDate":"2018年2月14日","excerpt":"\\n<p><a href=\\"https://scrapinghub.com/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">scrapinghub</a> 是一款爬虫托管平台。</p>\\n<h2>数据的价值</h2>\\n<p>更好的数据可带来更好的决策。</p>\\n<p>关注行业趋势，深入了解客户（和竞争对手），保护您的业务 - 可能性无穷无尽。</p>\\n<h2>按需数据</h2>\\n<p>如果您没有时间或专业知识来抓取网站，我们的网络抓取专家可以提供帮助。</p>\\n<p>你将掌握得很好。 我们是Scrapy的创建者和主要维护者，Scrapy是用Python编写的最流行的Web抓取框架。</p>","autoDesc":true}');export{d as comp,c as data};
